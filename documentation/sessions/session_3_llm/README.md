# Session 3 â€” IntÃ©gration LLM API

> **Date :** 2026-01-09  
> **Objectif :** CrÃ©er un endpoint `/chat` qui appelle une API LLM (GPT-4o) et sauvegarde les conversations

---

## ğŸ¯ Objectifs de la session

1. Comprendre ce qu'est une API LLM et comment l'appeler
2. CrÃ©er un module `backend/ai.py` pour appeler GitHub Models (GPT-4o)
3. GÃ©rer les secrets avec `.env` et `python-dotenv`
4. CrÃ©er l'endpoint POST `/chat` pour discuter avec le LLM
5. Modifier `memory.py` pour supporter les rÃ´les (user/assistant)
6. Sauvegarder les conversations complÃ¨tes dans SQLite

---

## ğŸ“š Concepts appris

### **API LLM : "Appeler un ami expert"**

- Une API LLM = service distant qui gÃ©nÃ¨re du texte intelligent
- DiffÃ©rence avec SQLite : DB = chercher info dÃ©jÃ  Ã©crite, LLM = gÃ©nÃ©rer nouvelle rÃ©ponse
- RequÃªte HTTP POST avec JSON (comme Swagger appelle notre FastAPI)
- NÃ©cessite une clÃ© API (token secret) pour authentification

### **GitHub Models : LLM gratuit pour Ã©tudiants**

- AccÃ¨s gratuit Ã  GPT-4o, Claude 3.5 Sonnet, Llama, Mistral via GitHub Education
- URL : https://github.com/marketplace/models
- Rate-limited mais suffisant pour apprendre
- Alternative parfaite : gratuit + sans utiliser ressources PC locales

### **Fichier `.env` : Stocker les secrets**

- Fichier pour variables d'environnement (clÃ©s API, mots de passe)
- Format : `NOM_VARIABLE=valeur` (sans espaces, sans guillemets)
- TOUJOURS dans `.gitignore` â†’ jamais committer les secrets
- BibliothÃ¨que `python-dotenv` pour lire le `.env`

### **Gestion d'erreurs avec `try/except`**

- Bloc `try:` â†’ code qui peut Ã©chouer (requÃªte HTTP)
- Bloc `except Exception as e:` â†’ que faire si erreur
- Pattern de rÃ©essai : boucle for + `time.sleep(2)` entre tentatives
- Message d'erreur poli si toutes les tentatives Ã©chouent

### **RÃ´les dans les conversations**

- `role="user"` â†’ messages de l'utilisateur
- `role="assistant"` â†’ rÃ©ponses du LLM
- Permet de reconstruire l'historique de conversation
- Essentiel pour contexte multi-tours (futures sessions)

---

## âœ… RÃ©alisations

### **1. Configuration `.env`**

```bash
GITHUB_TOKEN=ghp_...
MODEL_NAME=gpt-4o
```

### **2. Module `backend/ai.py` crÃ©Ã©**

Fonction principale : `demander_llm(prompt: str) -> str`

**FonctionnalitÃ©s :**

- Charge le token depuis `.env` avec `os.getenv()`
- Construit requÃªte HTTP vers GitHub Models API
- RÃ©essaie **3 fois** en cas d'Ã©chec (avec dÃ©lai 2 secondes)
- Retourne message d'erreur poli si Ã©chec total
- Extrait la rÃ©ponse du JSON : `resultat["choices"][0]["message"]["content"]`

### **3. Modifications `backend/memory.py`**

- âœ… Ajout colonne `role TEXT` dans la table SQLite
- âœ… `sauvegarder_message()` accepte paramÃ¨tre `role` (dÃ©faut : "user")
- âœ… `recuperer_messages()` retourne le champ `role` dans les dictionnaires
- âœ… DB recrÃ©Ã©e pour inclure la nouvelle colonne

### **4. Endpoint POST `/chat` dans `backend/main.py`**

- Nouveau modÃ¨le Pydantic : `ChatMessage(BaseModel)` avec champ `message: str`
- Import : `from backend.ai import demander_llm`

**Logique de l'endpoint :**

1. Recevoir question utilisateur
2. Sauvegarder avec `role="user"`
3. Appeler `demander_llm(message)`
4. Sauvegarder rÃ©ponse avec `role="assistant"`
5. Retourner `{"reponse": "..."}`

### **5. Tests rÃ©ussis**

âœ… **Test `backend/ai.py` seul :** GPT-4o rÃ©pond "Bonjour ! Comment puis-je vous aider ?"  
âœ… **Test POST `/chat` :** Question sur PowerShell â†’ rÃ©ponse complÃ¨te de GPT-4o  
âœ… **Test GET `/messages` :** 2 messages sauvegardÃ©s (user + assistant) avec rÃ´les corrects

---

## ğŸ› ï¸ Commandes utilisÃ©es

### Installation des dÃ©pendances

```powershell
pip install python-dotenv requests
```

### RecrÃ©er la DB avec colonne role

```powershell
rm memory.db
python -c "from backend.memory import initialiser_db; initialiser_db(); print('DB recrÃ©Ã©e')"
```

### Tester le module ai.py

```powershell
python backend/ai.py
```

### Lancer le serveur

```powershell
uvicorn backend.main:app --reload
```

### Tester avec Swagger

http://127.0.0.1:8000/docs

---

## ğŸ“‚ Fichiers crÃ©Ã©s/modifiÃ©s

### Nouveaux fichiers

- `backend/ai.py` â†’ Module LLM
- `.env` â†’ Configuration secrets (GITHUB_TOKEN, MODEL_NAME)

### Fichiers modifiÃ©s

- `backend/memory.py` â†’ Ajout support `role`
- `backend/main.py` â†’ Endpoint `/chat` + modÃ¨le `ChatMessage`
- `requirements.txt` â†’ Ajout `python-dotenv` et `requests`

---

## ğŸ§ª Exemple de conversation testÃ©e

**Question (via POST `/chat`) :**

```json
{ "message": "Comment lister les fichiers dans PowerShell ?" }
```

**RÃ©ponse de GPT-4o :**

```json
{
  "reponse": "Pour lister les fichiers dans PowerShell, vous pouvez utiliser la commande **`Get-ChildItem`** ou son alias **`ls`**..."
}
```

**Persistance vÃ©rifiÃ©e (GET `/messages`) :**

- Message 1 : `role="user"`, texte de la question
- Message 2 : `role="assistant"`, rÃ©ponse complÃ¨te de GPT-4o

---

## ğŸ“ Points d'apprentissage clÃ©s

### Ce qui a bien fonctionnÃ©

- âœ… L'utilisateur a **Ã©crit 100% du code** de `ai.py` lui-mÃªme (guidage pseudo-code)
- âœ… ComprÃ©hension rapide de l'analogie "appeler un ami expert"
- âœ… Bonne maÃ®trise de `try/except` et logique de rÃ©essai
- âœ… Tests mÃ©thodiques (module seul â†’ endpoint â†’ persistance)

### DifficultÃ©s rencontrÃ©es

- âš ï¸ PremiÃ¨re tentative : ajout `message` dans classe `Message` au lieu de crÃ©er `ChatMessage`
- âš ï¸ Correction rapide appliquÃ©e : sÃ©paration des 2 modÃ¨les Pydantic

### Ã‰volution depuis Session 2

- **Plus autonome** : Ã©crit des fonctions complÃ¨tes sans aide
- **Meilleure comprÃ©hension** des requÃªtes HTTP (POST, headers, JSON)
- **RÃ©flexes sÃ©curitÃ©** : comprend pourquoi `.env` dans `.gitignore`

---

## ğŸ”œ Prochaines Ã©tapes (Session 4)

### AmÃ©liorations possibles

1. **Contexte conversationnel** : envoyer les N derniers messages au LLM (mÃ©moire)
2. **Streaming** : afficher la rÃ©ponse mot par mot (SSE ou WebSocket)
3. **Frontend interactif** : interface chat dans `frontend/index.html`
4. **Gestion d'erreurs avancÃ©e** : timeout configurable, retry exponentiel
5. **Support multi-modÃ¨les** : basculer entre GPT-4o, Claude, Llama

---

## ğŸ“– Documentation associÃ©e

- **Guide technique dÃ©taillÃ© :** [GUIDE_TECHNIQUE.md](./GUIDE_TECHNIQUE.md)
- **Scripts finaux :** Dossier [scripts/](./scripts/)
- **Ã‰tat final :** [docs/chat_transitions/chat_4_session_3/CURRENT_STATE.md](../../chat_transitions/chat_4_session_3/CURRENT_STATE.md)
