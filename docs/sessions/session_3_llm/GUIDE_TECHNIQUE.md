# Guide Technique ‚Äî Session 3 : Int√©gration LLM API

> **Documentation d√©taill√©e ligne par ligne**  
> **Date :** 2026-01-09  
> **Niveau :** D√©butant Python

---

## üìã Table des mati√®res

1. [Configuration `.env`](#1-configuration-env)
2. [Module `backend/ai.py`](#2-module-backendaipy)
3. [Modifications `backend/memory.py`](#3-modifications-backendmemorypy)
4. [Modifications `backend/main.py`](#4-modifications-backendmainpy)
5. [Tests et v√©rifications](#5-tests-et-v√©rifications)

---

## 1. Configuration `.env`

### üìÑ Fichier `.env`

```bash
# Configuration API LLM
GITHUB_TOKEN=ghp_...
MODEL_NAME=gpt-4o
```

### üìñ Explications

**Ligne 1-2 : Commentaire**

- Les `#` en d√©but de ligne = commentaires (ignor√©s par Python)

**Ligne 3 : `GITHUB_TOKEN=ghp_...`**

- Format : `NOM_VARIABLE=valeur`
- **Pas d'espaces** autour du `=`
- **Pas de guillemets** autour de la valeur
- Le token commence par `ghp_` ou `github_pat_`

**Ligne 4 : `MODEL_NAME=gpt-4o`**

- Nom du mod√®le LLM √† utiliser
- Options : `gpt-4o`, `claude-3-5-sonnet`, `llama-3.1`, etc.

### üîí S√©curit√©

**Pourquoi `.env` dans `.gitignore` ?**

- Le token = mot de passe pour acc√©der √† l'API
- Si on commit `.env` ‚Üí tout le monde peut utiliser ton token
- Si quelqu'un vole ton token ‚Üí peut consommer ta limite gratuite

**Comment lire `.env` depuis Python ?**

```python
from dotenv import load_dotenv  # Importer la biblioth√®que
load_dotenv()  # Charger le fichier .env

token = os.getenv("GITHUB_TOKEN")  # Lire la variable
```

---

## 2. Module `backend/ai.py`

### üìÑ Code complet comment√©

```python
# Imports : biblioth√®ques n√©cessaires
import os          # Pour lire variables d'environnement (os.getenv)
import requests    # Pour faire des requ√™tes HTTP (comme fetch en JS)
import time        # Pour time.sleep() (attendre entre r√©essais)
from dotenv import load_dotenv  # Pour charger le fichier .env

# Charger les variables du fichier .env dans l'environnement
load_dotenv()


def demander_llm(prompt: str) -> str:
    """
    Appelle l'API GitHub Models (GPT-4o) pour obtenir une r√©ponse.
    R√©essaie 3 fois en cas d'erreur.

    Args:
        prompt: La question de l'utilisateur

    Returns:
        La r√©ponse du LLM ou un message d'erreur poli
    """

    # 1. R√©cup√©rer le token depuis .env
    token = os.getenv("GITHUB_TOKEN")

    # 2. V√©rifier que le token existe (s√©curit√©)
    if not token:
        return "Erreur : token GitHub manquant dans .env"

    # 3. Pr√©parer l'URL de l'API
    url = "https://models.inference.ai.azure.com/chat/completions"

    # 4. Pr√©parer les headers (informations HTTP)
    headers = {
        "Content-Type": "application/json",  # On envoie du JSON
        "Authorization": f"Bearer {token}"   # Authentification avec token
    }

    # 5. Pr√©parer les donn√©es √† envoyer (body de la requ√™te)
    donnees = {
        "model": os.getenv("MODEL_NAME", "gpt-4o"),  # Quel mod√®le utiliser
        "messages": [
            {"role": "user", "content": prompt}  # Format requis par l'API
        ]
    }

    # 6. Boucle pour r√©essayer 3 fois
    for tentative in range(1, 4):  # 1, 2, 3
        try:
            # 7. Envoyer la requ√™te POST
            reponse = requests.post(url, headers=headers, json=donnees)

            # 8. V√©rifier que la requ√™te a r√©ussi
            if reponse.status_code == 200:
                # 9. Extraire le texte de la r√©ponse
                resultat = reponse.json()
                texte_llm = resultat["choices"][0]["message"]["content"]
                return texte_llm
            else:
                # Afficher l'erreur dans la console
                print(f"Tentative {tentative} √©chou√©e : status {reponse.status_code}")

        except Exception as e:
            # Capturer toute erreur (timeout, connexion, etc.)
            print(f"Tentative {tentative} erreur : {e}")

        # 10. Attendre 2 secondes avant de r√©essayer
        if tentative < 3:
            time.sleep(2)

    # 11. Si les 3 tentatives √©chouent, message d'erreur poli
    return "D√©sol√©, le service est temporairement indisponible. Veuillez r√©essayer plus tard."
```

### üìñ Explications d√©taill√©es

#### **Bloc 1 : Imports**

```python
import os
import requests
import time
from dotenv import load_dotenv
```

- `os` : module standard Python pour syst√®me (fichiers, variables env)
- `requests` : biblioth√®que tierce pour HTTP (install√©e avec `pip install requests`)
- `time` : module standard pour `sleep()` (pause)
- `dotenv` : biblioth√®que pour lire `.env` (install√©e avec `pip install python-dotenv`)

#### **Bloc 2 : Charger `.env`**

```python
load_dotenv()
```

- Lit le fichier `.env` √† la racine du projet
- Charge les variables dans l'environnement (accessibles via `os.getenv()`)
- Si `.env` n'existe pas : aucune erreur, mais `os.getenv()` retournera `None`

#### **Bloc 3 : R√©cup√©rer le token**

```python
token = os.getenv("GITHUB_TOKEN")
if not token:
    return "Erreur : token GitHub manquant dans .env"
```

- `os.getenv("NOM")` : lire variable d'environnement
- Si `token = None` (pas trouv√©) ‚Üí `if not token:` est `True`
- `return` arr√™te la fonction imm√©diatement avec message d'erreur

#### **Bloc 4 : URL et headers**

```python
url = "https://models.inference.ai.azure.com/chat/completions"
headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {token}"
}
```

- `url` : adresse de l'API GitHub Models (Azure backend)
- `headers` : dictionnaire avec 2 cl√©s
  - `Content-Type` : dit au serveur "on envoie du JSON"
  - `Authorization` : prouve qu'on a le droit d'utiliser l'API
  - `f"Bearer {token}"` : f-string qui ins√®re la valeur de `token`

#### **Bloc 5 : Donn√©es √† envoyer**

```python
donnees = {
    "model": os.getenv("MODEL_NAME", "gpt-4o"),
    "messages": [
        {"role": "user", "content": prompt}
    ]
}
```

- `donnees` : dictionnaire Python (sera converti en JSON)
- `os.getenv("MODEL_NAME", "gpt-4o")` : valeur par d√©faut si variable absente
- `messages` : liste de dictionnaires (format requis par API OpenAI)
  - `"role": "user"` : ce message vient de l'utilisateur
  - `"content": prompt` : le texte de la question

#### **Bloc 6 : Boucle de r√©essais**

```python
for tentative in range(1, 4):  # 1, 2, 3
    try:
        # Code qui peut √©chouer
    except Exception as e:
        # Code ex√©cut√© si erreur
```

- `range(1, 4)` : g√©n√®re [1, 2, 3] (4 est exclu)
- `try:` : "essaie d'ex√©cuter ce code"
- `except Exception as e:` : "si erreur, ex√©cute ce bloc"
- `e` : variable qui contient l'objet erreur

#### **Bloc 7 : Requ√™te POST**

```python
reponse = requests.post(url, headers=headers, json=donnees)
```

- `requests.post()` : envoie requ√™te HTTP POST
- `url` : vers o√π envoyer
- `headers=headers` : ajouter headers HTTP
- `json=donnees` : convertit automatiquement dictionnaire en JSON et l'envoie

#### **Bloc 8 : V√©rifier succ√®s**

```python
if reponse.status_code == 200:
    resultat = reponse.json()
    texte_llm = resultat["choices"][0]["message"]["content"]
    return texte_llm
```

- `status_code == 200` : code HTTP pour "succ√®s"
- `reponse.json()` : convertit r√©ponse JSON en dictionnaire Python
- Navigation dictionnaire :
  - `resultat["choices"]` ‚Üí liste
  - `[0]` ‚Üí premier √©l√©ment
  - `["message"]` ‚Üí dictionnaire
  - `["content"]` ‚Üí texte final
- `return` sort de la fonction ET de la boucle

#### **Bloc 9 : Attendre avant r√©essai**

```python
if tentative < 3:
    time.sleep(2)
```

- Si on n'est pas √† la derni√®re tentative (3)
- Attendre 2 secondes avant de recommencer
- `time.sleep(2)` : pause de 2 secondes

#### **Bloc 10 : Message final**

```python
return "D√©sol√©, le service est temporairement indisponible..."
```

- Si on arrive ici : les 3 tentatives ont √©chou√©
- Retourne message poli au lieu de crasher

---

## 3. Modifications `backend/memory.py`

### üîÑ Changements apport√©s

#### **1. Ajout colonne `role` dans la table**

**Avant :**

```sql
CREATE TABLE IF NOT EXISTS messages (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    texte TEXT,
    nom_utilisateur TEXT,
    date_creation TEXT
)
```

**Apr√®s :**

```sql
CREATE TABLE IF NOT EXISTS messages (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    texte TEXT,
    nom_utilisateur TEXT,
    role TEXT,                    -- NOUVELLE COLONNE
    date_creation TEXT
)
```

**Pourquoi ?**

- Besoin de distinguer messages utilisateur vs r√©ponses LLM
- `role="user"` ‚Üí question de l'utilisateur
- `role="assistant"` ‚Üí r√©ponse du LLM

#### **2. Modification `sauvegarder_message()`**

**Avant :**

```python
def sauvegarder_message(texte, nom_utilisateur):
    # ...
    connexion.execute("""
        INSERT INTO messages (texte, nom_utilisateur, date_creation)
        VALUES (?, ?, ?)
    """, (texte, nom_utilisateur, date_maintenant))
```

**Apr√®s :**

```python
def sauvegarder_message(texte, nom_utilisateur, role="user"):  # Nouveau param√®tre
    # ...
    connexion.execute("""
        INSERT INTO messages (texte, nom_utilisateur, role, date_creation)
        VALUES (?, ?, ?, ?)                                      -- 4 placeholders
    """, (texte, nom_utilisateur, role, date_maintenant))       # 4 valeurs
```

**Explications :**

- `role="user"` : valeur par d√©faut si non fourni
- Ajout de `role` dans la liste des colonnes
- Ajout d'un `?` suppl√©mentaire (placeholder SQL)
- Ajout de `role` dans le tuple de valeurs

#### **3. Modification `recuperer_messages()`**

**Avant :**

```python
for ligne in lignes:
    message = {
        "id": ligne[0],
        "texte": ligne[1],
        "nom_utilisateur": ligne[2],
        "date_creation": ligne[3]      # Index 3
    }
```

**Apr√®s :**

```python
for ligne in lignes:
    message = {
        "id": ligne[0],
        "texte": ligne[1],
        "nom_utilisateur": ligne[2],
        "role": ligne[3],              # NOUVEAU : index 3
        "date_creation": ligne[4]      # Index d√©cal√© √† 4
    }
```

**Pourquoi les indices changent ?**

- SQLite retourne tuples : `(id, texte, nom_utilisateur, role, date_creation)`
- `ligne[0]` = 1er √©l√©ment (id)
- `ligne[3]` = 4√®me √©l√©ment (role)
- `ligne[4]` = 5√®me √©l√©ment (date_creation)

---

## 4. Modifications `backend/main.py`

### üÜï Nouveaut√©s ajout√©es

#### **1. Import de `demander_llm`**

```python
from backend.ai import demander_llm
```

- Importe la fonction depuis le module `ai.py`
- Maintenant accessible dans `main.py` avec `demander_llm()`

#### **2. Nouveau mod√®le Pydantic `ChatMessage`**

```python
class ChatMessage(BaseModel):
    message: str
```

**Pourquoi cr√©er un nouveau mod√®le ?**

- Endpoint `/chat` a besoin d'un format diff√©rent de `/message`
- `/message` : `{"texte": "...", "nom_utilisateur": "..."}`
- `/chat` : `{"message": "..."}`
- S√©paration des responsabilit√©s (principe SOLID)

#### **3. Endpoint POST `/chat`**

```python
@app.post("/chat")
def chat(msg: ChatMessage):
    # 1. Sauvegarder message utilisateur
    sauvegarder_message(msg.message, "Utilisateur", role="user")

    # 2. Appeler le LLM
    reponse_llm = demander_llm(msg.message)

    # 3. Sauvegarder r√©ponse LLM
    sauvegarder_message(reponse_llm, "Assistant", role="assistant")

    # 4. Retourner r√©ponse
    return {"reponse": reponse_llm}
```

### üìñ Explications ligne par ligne

**Ligne 1 : D√©corateur**

```python
@app.post("/chat")
```

- D√©clare un endpoint qui r√©pond aux requ√™tes POST
- URL : `http://127.0.0.1:8000/chat`

**Ligne 2 : D√©finition fonction**

```python
def chat(msg: ChatMessage):
```

- Nom fonction : `chat`
- Param√®tre `msg` : type `ChatMessage` (valid√© par Pydantic)
- Pydantic v√©rifie automatiquement que JSON contient `message`

**Ligne 3-4 : Sauvegarder question utilisateur**

```python
sauvegarder_message(msg.message, "Utilisateur", role="user")
```

- `msg.message` : acc√®de au champ `message` du JSON re√ßu
- `"Utilisateur"` : nom g√©n√©rique pour tous les utilisateurs
- `role="user"` : marque comme message utilisateur

**Ligne 5-6 : Appeler le LLM**

```python
reponse_llm = demander_llm(msg.message)
```

- Appelle la fonction de `backend/ai.py`
- Passe la question de l'utilisateur
- Stocke la r√©ponse dans `reponse_llm`

**Ligne 7-8 : Sauvegarder r√©ponse LLM**

```python
sauvegarder_message(reponse_llm, "Assistant", role="assistant")
```

- `reponse_llm` : texte g√©n√©r√© par GPT-4o
- `"Assistant"` : nom pour le LLM
- `role="assistant"` : marque comme r√©ponse LLM

**Ligne 9-10 : Retourner JSON**

```python
return {"reponse": reponse_llm}
```

- FastAPI convertit automatiquement dictionnaire en JSON
- Frontend recevra : `{"reponse": "Pour lister les fichiers..."}`

---

## 5. Tests et v√©rifications

### üß™ Test 1 : Module `ai.py` seul

**Commande :**

```powershell
python backend/ai.py
```

**Attendu :**

```
Bonjour ! Comment puis-je vous aider aujourd'hui ?
```

**Que se passe-t-il ?**

1. `load_dotenv()` charge `.env`
2. `demander_llm("Dis bonjour en une phrase")` est appel√©
3. Requ√™te HTTP envoy√©e √† GitHub Models
4. GPT-4o g√©n√®re une r√©ponse
5. R√©ponse affich√©e dans console

### üß™ Test 2 : Endpoint `/chat` via Swagger

**URL :** http://127.0.0.1:8000/docs

**JSON envoy√© :**

```json
{
  "message": "Comment lister les fichiers dans PowerShell ?"
}
```

**JSON re√ßu :**

```json
{
  "reponse": "Pour lister les fichiers dans PowerShell, vous pouvez utiliser la commande **`Get-ChildItem`**..."
}
```

**Flux d'ex√©cution :**

1. Swagger envoie POST √† `/chat`
2. FastAPI valide JSON avec Pydantic
3. `chat()` appelle `sauvegarder_message()` (role="user")
4. `chat()` appelle `demander_llm()`
5. API GitHub Models traite la requ√™te (~2-3 secondes)
6. GPT-4o g√©n√®re r√©ponse compl√®te
7. `chat()` appelle `sauvegarder_message()` (role="assistant")
8. FastAPI retourne JSON au frontend

### üß™ Test 3 : V√©rifier persistance

**Endpoint :** GET `/messages`

**R√©sultat attendu :**

```json
{
  "messages": [
    {
      "id": 1,
      "texte": "Comment lister les fichiers dans PowerShell ?",
      "nom_utilisateur": "Utilisateur",
      "role": "user",
      "date_creation": "2026-01-09T10:41:09.043304"
    },
    {
      "id": 2,
      "texte": "Pour lister les fichiers dans PowerShell...",
      "nom_utilisateur": "Assistant",
      "role": "assistant",
      "date_creation": "2026-01-09T10:41:13.371480"
    }
  ],
  "total": 2
}
```

**V√©rifications :**

- ‚úÖ 2 messages sauvegard√©s
- ‚úÖ `role` pr√©sent et correct (user/assistant)
- ‚úÖ Dates diff√©rentes (quelques secondes d'√©cart)
- ‚úÖ Ordre pr√©serv√© (user puis assistant)

---

## üéì Concepts cl√©s √† retenir

### 1. **API REST = communication client-serveur**

- Client (frontend) envoie JSON
- Serveur (backend) traite et r√©pond JSON
- Notre backend appelle aussi une autre API (GitHub Models)

### 2. **Gestion d'erreurs robuste**

- Toujours v√©rifier que les ressources existent (token, fichiers)
- `try/except` pour code qui peut √©chouer
- R√©essayer en cas d'erreur temporaire (r√©seau)
- Messages d'erreur polis pour l'utilisateur

### 3. **S√©paration des responsabilit√©s**

- `ai.py` : communication avec LLM
- `memory.py` : persistance SQLite
- `main.py` : orchestration des endpoints
- Chaque module a un r√¥le pr√©cis

### 4. **S√©curit√© des secrets**

- Jamais de cl√©s API en dur dans le code
- `.env` pour stocker secrets localement
- `.gitignore` pour ne pas committer `.env`
- `python-dotenv` pour charger variables

### 5. **R√¥les conversationnels**

- Distinction user/assistant essentielle
- Permet de reconstruire historique
- Pr√©pare contexte multi-tours (sessions futures)

---

## üìö Ressources compl√©mentaires

### Documentation officielle

- **GitHub Models :** https://github.com/marketplace/models
- **Requests :** https://docs.python-requests.org
- **Python-dotenv :** https://pypi.org/project/python-dotenv/
- **FastAPI :** https://fastapi.tiangolo.com

### Commandes utiles

```powershell
# Lister packages install√©s
pip list

# Voir d√©tails d'un package
pip show requests

# Mettre √† jour requirements.txt
pip freeze > requirements.txt

# Installer depuis requirements.txt
pip install -r requirements.txt
```

---

**üìå Fin du guide technique**  
**Prochaine √©tape :** Frontend interactif (Session 4)
