# Ã‰tat actuel â€” Fin de Session 3

> **Date :** 2026-01-09  
> **Chat :** 4  
> **Session :** 3 â€” IntÃ©gration LLM API

---

## ğŸ¯ Ce qui a Ã©tÃ© accompli

### âœ… Concepts appris

- **API LLM** : Service distant qui gÃ©nÃ¨re du texte intelligent
- **Analogie "appeler un ami expert"** : LLM = expert au tÃ©lÃ©phone vs SQLite = bibliothÃ¨que
- **GitHub Models** : API LLM gratuite pour Ã©tudiants (GPT-4o, Claude, Llama)
- **Fichier `.env`** : Stocker secrets (clÃ©s API) sans les committer
- **`python-dotenv`** : BibliothÃ¨que pour charger variables depuis `.env`
- **`requests.post()`** : Envoyer requÃªtes HTTP (comme `fetch()` en JavaScript)
- **`try/except`** : GÃ©rer les erreurs qui peuvent survenir (rÃ©seau, timeout)
- **Pattern de rÃ©essai** : Boucle + `time.sleep(2)` entre tentatives
- **RÃ´les conversationnels** : `role="user"` vs `role="assistant"`
- **Headers HTTP** : `Authorization: Bearer token`, `Content-Type: application/json`

### âœ… Code Ã©crit

#### **Nouveau fichier : `.env`**

```bash
GITHUB_TOKEN=ghp_...
MODEL_NAME=gpt-4o
```

- Configuration secrets (JAMAIS committer)
- DÃ©jÃ  dans `.gitignore` âœ…

#### **Nouveau module : `backend/ai.py`**

Fonction principale : `demander_llm(prompt: str) -> str`

**FonctionnalitÃ©s :**

```python
def demander_llm(prompt: str) -> str:
    # 1. Charger token depuis .env
    token = os.getenv("GITHUB_TOKEN")

    # 2. VÃ©rifier token existe
    if not token:
        return "Erreur : token manquant"

    # 3. PrÃ©parer requÃªte HTTP (URL, headers, JSON)
    url = "https://models.inference.ai.azure.com/chat/completions"
    headers = {"Authorization": f"Bearer {token}", ...}
    donnees = {"model": "gpt-4o", "messages": [...]}

    # 4. Boucle rÃ©essai (3 tentatives)
    for tentative in range(1, 4):
        try:
            reponse = requests.post(url, headers=headers, json=donnees)
            if reponse.status_code == 200:
                return resultat["choices"][0]["message"]["content"]
        except Exception as e:
            print(f"Erreur : {e}")
        time.sleep(2)  # Attendre avant rÃ©essai

    # 5. Message poli si Ã©chec total
    return "DÃ©solÃ©, service indisponible..."
```

#### **Modifications : `backend/memory.py`**

**1. Table SQLite modifiÃ©e (colonne `role` ajoutÃ©e)**

```sql
CREATE TABLE IF NOT EXISTS messages (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    texte TEXT,
    nom_utilisateur TEXT,
    role TEXT,              -- NOUVEAU
    date_creation TEXT
)
```

**2. Fonction `sauvegarder_message()` modifiÃ©e**

```python
def sauvegarder_message(texte, nom_utilisateur, role="user"):
    # ...
    connexion.execute("""
        INSERT INTO messages (texte, nom_utilisateur, role, date_creation)
        VALUES (?, ?, ?, ?)
    """, (texte, nom_utilisateur, role, date_maintenant))
```

**3. Fonction `recuperer_messages()` modifiÃ©e**

```python
for ligne in lignes:
    message = {
        "id": ligne[0],
        "texte": ligne[1],
        "nom_utilisateur": ligne[2],
        "role": ligne[3],              # NOUVEAU
        "date_creation": ligne[4]
    }
```

#### **Modifications : `backend/main.py`**

**1. Import ajoutÃ©**

```python
from backend.ai import demander_llm
```

**2. Nouveau modÃ¨le Pydantic**

```python
class ChatMessage(BaseModel):
    message: str
```

**3. Nouveau endpoint POST `/chat`**

```python
@app.post("/chat")
def chat(msg: ChatMessage):
    # 1. Sauvegarder message user
    sauvegarder_message(msg.message, "Utilisateur", role="user")

    # 2. Appeler LLM
    reponse_llm = demander_llm(msg.message)

    # 3. Sauvegarder rÃ©ponse LLM
    sauvegarder_message(reponse_llm, "Assistant", role="assistant")

    # 4. Retourner JSON
    return {"reponse": reponse_llm}
```

### âœ… Documentation crÃ©Ã©e

- `docs/sessions/session_3_llm/README.md` : Vue d'ensemble âœ…
- `docs/sessions/session_3_llm/GUIDE_TECHNIQUE.md` : Explications ligne par ligne âœ…
- `docs/sessions/session_3_llm/scripts/` : Copies finales (main.py, memory.py, ai.py) âœ…
- `docs/chat_transitions/chat_4_session_3/CURRENT_STATE.md` : Ce fichier âœ…

### âœ… Tests rÃ©ussis

#### Test 1 : Module `ai.py` seul âœ…

**Commande :**

```powershell
python backend/ai.py
```

**RÃ©sultat :**

```
Bonjour ! Comment puis-je vous aider aujourd'hui ?
```

#### Test 2 : Endpoint POST `/chat` âœ…

**Swagger** : http://127.0.0.1:8000/docs

**Envoi :**

```json
{ "message": "Comment lister les fichiers dans PowerShell ?" }
```

**RÃ©ponse :**

```json
{
  "reponse": "Pour lister les fichiers dans PowerShell, vous pouvez utiliser la commande **`Get-ChildItem`** ou son alias **`ls`**..."
}
```

#### Test 3 : Persistance SQLite âœ…

**Endpoint GET `/messages`** :

```json
{
  "messages": [
    {
      "id": 1,
      "texte": "Comment lister les fichiers dans PowerShell ?",
      "nom_utilisateur": "Utilisateur",
      "role": "user",
      "date_creation": "2026-01-09T10:41:09.043304"
    },
    {
      "id": 2,
      "texte": "Pour lister les fichiers dans PowerShell...",
      "nom_utilisateur": "Assistant",
      "role": "assistant",
      "date_creation": "2026-01-09T10:41:13.371480"
    }
  ],
  "total": 2
}
```

---

## ğŸ“ Structure actuelle du projet

```
os-assistant/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py           âœ… ModifiÃ© : endpoint /chat + import demander_llm
â”‚   â”œâ”€â”€ memory.py         âœ… ModifiÃ© : support role (user/assistant)
â”‚   â”œâ”€â”€ ai.py             âœ… NOUVEAU : appel API GitHub Models
â”‚   â””â”€â”€ __pycache__/
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ index.html        (pas encore modifiÃ©)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ INDEX.md
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ sessions/
â”‚   â”‚   â”œâ”€â”€ session_0_setup/
â”‚   â”‚   â”œâ”€â”€ session_1_pydantic/
â”‚   â”‚   â”œâ”€â”€ session_2_sqlite/
â”‚   â”‚   â””â”€â”€ session_3_llm/       âœ… NOUVEAU
â”‚   â”‚       â”œâ”€â”€ README.md
â”‚   â”‚       â”œâ”€â”€ GUIDE_TECHNIQUE.md
â”‚   â”‚       â””â”€â”€ scripts/
â”‚   â”‚           â”œâ”€â”€ main.py
â”‚   â”‚           â”œâ”€â”€ memory.py
â”‚   â”‚           â””â”€â”€ ai.py
â”‚   â””â”€â”€ chat_transitions/
â”‚       â”œâ”€â”€ chat_1_session_0/
â”‚       â”œâ”€â”€ chat_2_session_1/
â”‚       â”œâ”€â”€ chat_3_session_2/
â”‚       â””â”€â”€ chat_4_session_3/    âœ… NOUVEAU
â”‚           â””â”€â”€ CURRENT_STATE.md (ce fichier)
â”œâ”€â”€ .env                  âœ… NOUVEAU (secrets)
â”œâ”€â”€ .gitignore            âœ… Contient .env
â”œâ”€â”€ memory.db             âœ… RecrÃ©Ã© avec colonne role
â”œâ”€â”€ requirements.txt      (Ã  mettre Ã  jour)
â””â”€â”€ README.md
```

---

## ğŸ”§ Commandes exÃ©cutÃ©es

### Installation bibliothÃ¨ques

```powershell
pip install python-dotenv requests
```

### RecrÃ©er DB avec colonne role

```powershell
rm memory.db
python -c "from backend.memory import initialiser_db; initialiser_db()"
```

### Tester module ai.py

```powershell
python backend/ai.py
```

### Lancer serveur

```powershell
uvicorn backend.main:app --reload
```

---

## ğŸ“Š Statistiques

- **Fichiers crÃ©Ã©s** : 2 (ai.py, .env)
- **Fichiers modifiÃ©s** : 2 (memory.py, main.py)
- **Lignes de code ajoutÃ©es** : ~120
- **Endpoints totaux** : 4 (ping, message, messages, **chat**)
- **Nouveaux concepts** : 10+ (API, try/except, .env, roles, etc.)
- **Tests rÃ©ussis** : 3/3 âœ…

---

## ğŸ“ Apprentissages clÃ©s de la session

### RÃ©ussites majeures

1. âœ… **ComprÃ©hension API LLM** : analogie "ami expert" trÃ¨s efficace
2. âœ… **Code Ã©crit 100% par l'utilisateur** : guidage pseudo-code â†’ code rÃ©el
3. âœ… **Gestion erreurs robuste** : pattern rÃ©essai + messages polis
4. âœ… **SÃ©curitÃ© secrets** : `.env` + `.gitignore` bien compris
5. âœ… **Tests mÃ©thodiques** : module seul â†’ endpoint â†’ persistance

### Ã‰volution depuis Session 2

- **Plus autonome** : Ã©crit fonctions complÃ¨tes (50+ lignes) sans aide
- **Meilleure comprÃ©hension HTTP** : POST, headers, JSON, status codes
- **RÃ©flexes sÃ©curitÃ©** : pourquoi ne pas committer secrets
- **DÃ©bogage efficace** : teste Ã©tape par Ã©tape (isolation problÃ¨mes)

### Points d'attention

- âš ï¸ PremiÃ¨re tentative : ajout `message` dans classe `Message` au lieu de crÃ©er `ChatMessage`
- âœ… Correction immÃ©diate : sÃ©paration modÃ¨les Pydantic

---

## ğŸš€ Prochaines Ã©tapes possibles

### Session 4 : Frontend interactif

**Objectifs :**

- CrÃ©er interface chat dans `frontend/index.html`
- JavaScript pour appeler `/chat` avec `fetch()`
- Afficher conversation en temps rÃ©el
- Design simple avec CSS

### AmÃ©liorations futures

1. **Contexte conversationnel** : envoyer historique au LLM (mÃ©moire multi-tours)
2. **Streaming** : afficher rÃ©ponse mot par mot (SSE ou WebSocket)
3. **Gestion d'erreurs avancÃ©e** : retry exponentiel, timeout configurable
4. **Support multi-modÃ¨les** : switcher entre GPT-4o, Claude, Llama
5. **Interface admin** : gÃ©rer conversations, effacer historique
6. **DÃ©ploiement** : Render, Heroku, ou GitHub Pages (frontend)

---

## ğŸ“ TODO avant Session 4

- [ ] Mettre Ã  jour `requirements.txt` (ajouter `python-dotenv` et `requests`)
- [ ] Commenter code si besoin
- [ ] Tester endpoint `/chat` avec plusieurs questions
- [ ] VÃ©rifier que persistance fonctionne aprÃ¨s redÃ©marrage serveur

---

## ğŸ”— Liens utiles

- **GitHub Models** : https://github.com/marketplace/models
- **Documentation Requests** : https://docs.python-requests.org
- **Documentation Python-dotenv** : https://pypi.org/project/python-dotenv/
- **FastAPI Docs** : https://fastapi.tiangolo.com

---

**ğŸ“Œ Session 3 terminÃ©e avec succÃ¨s !**  
**Prochain chat : Session 4 â€” Frontend interactif**
